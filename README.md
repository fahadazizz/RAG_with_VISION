# RAG Chatbot Pipeline

A complete end-to-end Retrieval-Augmented Generation (RAG) chatbot system built with LangChain, Pinecone, and Ollama.

## ğŸŒŸ Features

- **Document Ingestion**: Upload PDF, DOCX files or ingest content from URLs
- **Smart Chunking**: Intelligent text splitting with context preservation
- **Vector Storage**: Pinecone for scalable vector storage
- **Local LLM**: Ollama for privacy-focused generation
- **REST API**: FastAPI endpoints for programmatic access
- **Web Dashboard**: Beautiful Streamlit interface

## ğŸ—ï¸ Architecture

```
User Document â†’ Load â†’ Clean â†’ Chunk â†’ Embed â†’ Pinecone
                                                    â†“
User Query â†’ Encode â†’ Retrieve â†’ Rerank â†’ Augment â†’ LLM â†’ Response
```

## ğŸ“‹ Prerequisites

1. **Python 3.12+**
2. **Ollama** installed and running
   ```bash
   # Install Ollama from https://ollama.ai
   ollama serve
   ollama pull kimi-k2:thinking  # or your preferred model
   ```
3. **Pinecone Account** with API key and index created

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
cd /Volumes/DataDrive/my_AGENTS_AND_MCP/rag_wtih_CLIP
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure Environment

Create/update `.env` file:
```env
PINECONE_API_KEY=your_api_key_here
PINECONE_INDEX_NAME=your_index_name
```

### 3. Run the System

**Option A: API + Dashboard (Recommended)**

Terminal 1 - Start API:
```bash
source .venv/bin/activate
uvicorn api:app --reload
```

Terminal 2 - Start Dashboard:
```bash
source .venv/bin/activate
streamlit run dashboard.py
```

Then open http://localhost:8501 in your browser.

**Option B: Programmatic Usage**

```python
from agents.document_agent import get_document_agent
from chains.rag_chain import get_rag_chain

# Ingest a document
agent = get_document_agent()
result = agent.ingest_file("path/to/document.pdf")
print(f"Created {result['chunks_created']} chunks")

# Query the system
rag = get_rag_chain()
response = rag.query("What is this document about?")
print(response.answer)
```

## ğŸ“ Project Structure

```
rag_wtih_CLIP/
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ api.py                 # FastAPI endpoints
â”œâ”€â”€ dashboard.py           # Streamlit UI
â”œâ”€â”€ test_pipeline.py       # Component tests
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ document_agent.py  # Document ingestion orchestrator
â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ retriever.py       # Retrieval with reranking
â”‚   â””â”€â”€ rag_chain.py       # Complete RAG chain
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ embedding_model.py # Sentence Transformers
â”‚   â”œâ”€â”€ vector_store.py    # Pinecone manager
â”‚   â””â”€â”€ llm.py             # Ollama wrapper
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ rag_prompts.py     # Engineered prompts
â””â”€â”€ tools/
    â””â”€â”€ utils/
        â”œâ”€â”€ text_cleaner.py     # Text preprocessing
        â”œâ”€â”€ text_chunker.py     # Smart chunking
        â””â”€â”€ document_loaders.py # PDF/DOCX/URL loaders
```

## âš™ï¸ Configuration

Edit `config.py` or set environment variables:

| Setting | Default | Description |
|---------|---------|-------------|
| `embedding_model_name` | `sentence-transformers/all-MiniLM-L6-v2` | Embedding model |
| `llm_model_name` | `kimi-k2:thinking` | Ollama model |
| `chunk_size` | `1000` | Characters per chunk |
| `chunk_overlap` | `200` | Overlap between chunks |
| `rag_top_k` | `5` | Initial retrieval count |
| `rag_score_threshold` | `0.5` | Minimum similarity score |
| `rag_rerank_top_k` | `3` | Final context documents |

## ğŸ”Œ API Endpoints

### Upload Document
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

### Ingest URL
```bash
curl -X POST "http://localhost:8000/upload-url" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/article"}'
```

### Query
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic?"}'
```

## ğŸ§ª Testing

Run the test suite to verify all components:

```bash
source .venv/bin/activate
python test_pipeline.py
```

## ğŸ¯ Key Features Explained

### Smart Text Processing
- **Cleaning**: Removes URLs, emails, normalizes whitespace
- **Chunking**: Recursive splitting with configurable overlap
- **Context**: Optional context windows from neighboring chunks

### Intelligent Retrieval
- **Vector Search**: Semantic similarity via Pinecone
- **Score Filtering**: Removes low-relevance results
- **Reranking**: Selects top-k most relevant chunks

### Anti-Hallucination Prompts
- Strict source-based answering
- Explicit acknowledgment of limitations
- Source citation requirements

## ğŸ› ï¸ Troubleshooting

**Ollama Connection Error**
```bash
# Make sure Ollama is running
ollama serve

# Check if model is available
ollama list
```

**Pinecone Connection Error**
- Verify API key in `.env`
- Check index name matches your Pinecone dashboard
- Ensure index dimension matches embedding model (384 for all-MiniLM-L6-v2)

**Import Errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```

## ğŸ“š Tech Stack

- **LangChain**: Orchestration framework
- **Pinecone**: Vector database
- **Ollama**: Local LLM inference
- **Sentence Transformers**: Embeddings
- **FastAPI**: REST API
- **Streamlit**: Web dashboard
- **Pydantic**: Configuration management

## ğŸ¤ Contributing

This is a complete RAG implementation following best practices:
- Modular architecture
- Type hints throughout
- Comprehensive error handling
- Engineered prompts for quality
- Configurable components

## ğŸ“„ License

MIT License - feel free to use and modify!

## ğŸ“ Learn More

- [LangChain Documentation](https://python.langchain.com/)
- [Pinecone Documentation](https://docs.pinecone.io/)
- [Ollama Documentation](https://ollama.ai/docs)
