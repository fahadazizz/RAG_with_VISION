# ğŸ”® VRAG - Vision RAG Pipeline

A **multimodal Retrieval-Augmented Generation** system that understands both **text and images**. Built with CLIP for vision embeddings, LangChain for orchestration, Pinecone for vector storage, and Ollama for local LLM inference.

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TB
    subgraph User Interface
        Dashboard["ğŸ–¥ï¸ Streamlit Dashboard"]
        API["âš¡ FastAPI Server"]
    end
    
    Dashboard --> API
    
    subgraph Ingestion Pipeline
        direction TB
        Upload["ğŸ“„ Upload\nPDF / DOCX / Image"]
        Extract["ğŸ“ Extract Text + Images\nPyMuPDF + RapidOCR"]
        Chunk["âœ‚ï¸ Chunk Text\nRecursiveCharacterTextSplitter"]
        
        subgraph Embeddings
            TextEmbed["ğŸ”¤ Text Embedding\nembeddinggemma (768-d)"]
            CLIPEmbed["ğŸ–¼ï¸ Image Embedding\nCLIP ViT-L/14 (768-d)"]
            CLIPLabel["ğŸ·ï¸ Zero-Shot Label\nCLIP Classification"]
        end
        
        Upload --> Extract
        Extract --> Chunk
        Extract --> CLIPEmbed
        Extract --> CLIPLabel
        Chunk --> TextEmbed
        
        TextEmbed --> VectorDB["ğŸ—„ï¸ Pinecone\nUnified 768-d Index"]
        CLIPEmbed --> VectorDB
    end
    
    API --> Upload
    
    subgraph Query Pipeline
        direction TB
        Query["â“ User Query\nText / Image / Both"]
        
        subgraph Fusion
            QueryText["ğŸ”¤ Text â†’ embeddinggemma"]
            QueryImage["ğŸ–¼ï¸ Image â†’ CLIP"]
            Fuse["âš—ï¸ Normalize + Fuse\n(v_text + v_image) / 2"]
        end
        
        Query --> QueryText
        Query --> QueryImage
        QueryText --> Fuse
        QueryImage --> Fuse
        
        Fuse --> Search["ğŸ” Vector Search\nCosine Similarity"]
        Search --> VectorDB
        VectorDB --> Context["ğŸ“‹ Context Builder\nText + Image Metadata"]
        Context --> Prompt["ğŸ“ VRAG Prompt\nMultimodal Instructions"]
        Prompt --> LLM["ğŸ¤– Local LLM\nOllama"]
        LLM --> Response["ğŸ’¬ Answer + Sources"]
    end
    
    API --> Query
    Response --> API
```

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **ğŸ–¼ï¸ Vision Understanding** | CLIP embeds images into the same 768-d space as text |
| **ğŸ·ï¸ Zero-Shot Classification** | Automatically labels images (chart, diagram, table, etc.) |
| **âš—ï¸ Multimodal Fusion** | Query with text, image, or both - vectors are normalized and fused |
| **ğŸ“„ PDF + OCR** | Extracts text and images from PDFs using PyMuPDF + RapidOCR |
| **ğŸ”’ Local AI** | Uses Ollama for embeddings and LLM - no data leaves your machine |
| **ğŸ¯ Unified Index** | Single Pinecone index stores both text and image vectors |

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Text Embeddings** | `embeddinggemma:latest` via Ollama (768-d) |
| **Image Embeddings** | `openai/clip-vit-large-patch14` (768-d) |
| **LLM** | Ollama (configurable model) |
| **Vector Database** | Pinecone |
| **Backend** | FastAPI |
| **Frontend** | Streamlit |
| **Document Parsing** | PyMuPDFLoader + RapidOCR |

---

## ğŸ“¦ Installation

```bash
# Clone and enter directory
git clone <repository-url>
cd rag_with_clip

# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cat > .env << EOF
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_index_name
EOF
```

### Prerequisites
- Python 3.10+
- [Ollama](https://ollama.ai) installed with `embeddinggemma` model
- Pinecone account with a **768-dimension** index

---

## ğŸš€ Usage

### Start the Services

```bash
# Terminal 1: API Server
uvicorn api:app --reload

# Terminal 2: Dashboard
streamlit run dashboard.py
```

- **API**: http://localhost:8000
- **Dashboard**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/upload` | POST | Ingest PDF/DOCX (text + images) |
| `/upload-image` | POST | Ingest standalone image |
| `/chat` | POST | Multimodal query (text, image, or both) |

### Query Modes

The dashboard supports three query modes:

1. **ğŸ“ Text Only**: Traditional text-based retrieval
2. **ğŸ–¼ï¸ Image Only**: Find similar images using CLIP
3. **ğŸ“+ğŸ–¼ï¸ Text + Image**: Fused multimodal search

---

## ğŸ“‚ Project Structure

```
rag_with_clip/
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ document_agent.py    # Orchestrates file ingestion
â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ rag_chain.py         # Query â†’ Retrieve â†’ Generate
â”‚   â””â”€â”€ retriever.py         # Multimodal vector search
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ clip_model.py        # CLIP wrapper (embed + label)
â”‚   â”œâ”€â”€ embedding_model.py   # Ollama text embeddings
â”‚   â””â”€â”€ llm.py               # Ollama LLM wrapper
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ rag_prompts.py       # VRAG system prompt
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ process_document.py  # Processing pipeline
â”‚   â”œâ”€â”€ load_documents.py    # Document loader interface
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ document_loaders.py  # PDF/DOCX loaders
â”‚       â”œâ”€â”€ text_cleaner.py      # Text normalization
â”‚       â””â”€â”€ text_chunker.py      # Recursive chunking
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ vector_store.py      # Pinecone + multimodal search
â”œâ”€â”€ api.py                   # FastAPI application
â”œâ”€â”€ dashboard.py             # Streamlit UI
â”œâ”€â”€ config.py                # Settings
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Configuration

Edit `config.py` or set environment variables:

| Setting | Default | Description |
|---------|---------|-------------|
| `RAG_TOP_K` | 5 | Documents to retrieve |
| `CHUNK_SIZE` | 1000 | Text chunk size |
| `CHUNK_OVERLAP` | 200 | Overlap between chunks |
| `EMBEDDING_MODEL_NAME` | embeddinggemma:latest | Ollama embedding model |

---

## ğŸ”„ How It Works

### Ingestion Flow
1. **Upload** PDF/DOCX/Image via API or dashboard
2. **Extract** text (with OCR) and images from documents
3. **Embed** text chunks with `embeddinggemma` (768-d)
4. **Embed** images with CLIP (768-d) + generate zero-shot labels
5. **Store** all vectors in unified Pinecone index

### Query Flow
1. **Receive** query (text, image, or both)
2. **Embed** text with `embeddinggemma`, image with CLIP
3. **Fuse** embeddings: `normalize((v_text + v_image) / 2)`
4. **Search** Pinecone for similar vectors
5. **Build** context from retrieved text + image metadata
6. **Generate** answer using local LLM with VRAG prompt
7. **Return** answer with source citations

---

## ğŸ“ License

MIT License - See LICENSE file for details.
